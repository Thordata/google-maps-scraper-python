# src/scraper.py
import os
import logging
import json
from typing import List, Dict, Any, Union
from thordata import ThordataClient
from .config import SPIDER_CONFIG, DEFAULT_TIMEOUT, POLL_INTERVAL

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger("GoogleMapsScraper")

class GoogleMapsScraper:
    def __init__(self):
        self.api_key = os.getenv("THORDATA_SCRAPER_TOKEN")
        # Task API éœ€è¦ public token
        self.public_token = os.getenv("THORDATA_PUBLIC_TOKEN")
        self.public_key = os.getenv("THORDATA_PUBLIC_KEY")
        
        if not self.api_key:
            raise ValueError("THORDATA_SCRAPER_TOKEN is required in .env")

        self.client = ThordataClient(
            scraper_token=self.api_key,
            public_token=self.public_token,
            public_key=self.public_key
        )

    def _run_task_mode(self, mode: str, params: Dict[str, Any]) -> Dict:
        """è¿è¡Œ Web Scraper Task (ç”¨äº Details å’Œ Reviews)"""
        cfg = SPIDER_CONFIG[mode]
        logger.info(f"ğŸš€ Starting Task: {cfg['desc']} (ID: {cfg['id']})")
        
        try:
            # è‡ªåŠ¨è½®è¯¢ç›´åˆ°å®Œæˆ
            result_url = self.client.run_task(
                file_name=f"gmaps_{mode}_{os.getpid()}",
                spider_id=cfg["id"],
                spider_name=cfg["name"],
                parameters=params,
                max_wait=DEFAULT_TIMEOUT,
                initial_poll_interval=POLL_INTERVAL
            )
            
            logger.info(f"âœ… Task Finished. Downloading data...")
            
            # ä¸‹è½½å¹¶è§£æ JSON
            import requests
            response = requests.get(result_url)
            try:
                return response.json()
            except:
                return {"raw_data": response.text}
                
        except Exception as e:
            logger.error(f"âŒ Task Failed: {e}")
            return {"error": str(e)}

    def search_businesses(self, keyword: str, limit: int = 20) -> Dict:
        """
        [Hybrid Mode] ä½¿ç”¨ SERP API è¿›è¡Œæœç´¢ (é€Ÿåº¦å¿«)
        """
        logger.info(f"ğŸ” Searching via SERP API: '{keyword}'")
        try:
            # ä½¿ç”¨ SDK çš„ SERP æ¥å£
            # engine="google_maps" æ˜¯ SDK æ ‡å‡†æ”¯æŒçš„
            data = self.client.serp_search(
                query=keyword,
                engine="google_maps",
                type="search",
                num=limit
            )
            return data
        except Exception as e:
            logger.error(f"âŒ Search Failed: {e}")
            return {"error": str(e)}

    def get_details(self, url: str) -> Dict:
        """
        ä½¿ç”¨ Web Scraper è·å–è¯¦æƒ…
        Param: url (Google Maps URL)
        """
        # å¯¹åº” cURL: {"url": "..."}
        params = {"url": url}
        return self._run_task_mode("details", params)

    def get_reviews(self, url: str, days_limit: int = 0) -> Dict:
        """
        ä½¿ç”¨ Web Scraper è·å–è¯„è®º
        Param: url, days_limit (Optional)
        """
        # å¯¹åº” cURL: {"url": "...", "days_limit": "10"}
        params = {"url": url}
        if days_limit > 0:
            params["days_limit"] = str(days_limit)
            
        return self._run_task_mode("reviews", params)